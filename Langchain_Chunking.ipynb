{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34bddd5b-ff5b-46a3-8ffe-9622ba178950",
   "metadata": {},
   "source": [
    "## Chunking:\n",
    "\n",
    "- Different LLM models have different token limit, we can not process millions of tokens at a time\n",
    "\n",
    "- when we load a large text, will split it into smaller pieces is called as **Chunks**\n",
    "\n",
    "- These chunks are then Embedded(numerical representation) and stored in a vector database \n",
    "\n",
    "- So that relevant chunks can be retrived \n",
    "\n",
    "- Types of Chunking methods\n",
    "\n",
    "    - **CharcterTextSplitter**\n",
    "\n",
    "    - **RecursiveCharcterTextSplitter**\n",
    "\n",
    "    - **TokenTextSplitter**\n",
    "\n",
    "    - **SemanticChunking**\n",
    "\n",
    "    - NLTKTextSplitter\n",
    "\n",
    "    - MarkDownTextSplitter\n",
    "\n",
    "    - SpacyTextSplitter\n",
    "\n",
    "    - PythonTextSplitter\n",
    "\n",
    "    - HTMLTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb7f5f-071c-4499-9f2e-fc62820d2071",
   "metadata": {},
   "source": [
    "**CharcterTextSplitter**\n",
    "\n",
    "- Just split based on charcters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329c6750-2604-4a7c-9446-0dda7375ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805452cd-ce10-4509-aff5-53ebd8cdaa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\n",
    "It provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \n",
    "LangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \n",
    "This chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81bf1ec-ce4c-4f03-9130-983b9929a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "Splitter=CharacterTextSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11582ae0-5b79-4135-8afb-ca599b4d46ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\\nIt provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \\nLangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \\nThis chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb728f6f-3ca8-43db-9006-91423c479f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\\nIt provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \\nLangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \\nThis chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Splitter=CharacterTextSplitter(separator='\\n')\n",
    "Splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96f8a6e8-64a6-4e8a-afca-a1d7a237acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Splitter.split_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b64f326d-52e0-4d02-8e03-c741aa03e04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\\nIt provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \\nLangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \\nThis chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Splitter=CharacterTextSplitter(separator='')\n",
    "Splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90c17cb1-38bc-41bb-a0cf-f1dbfb7b456d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('LangChain is an open')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85000e94-12f6-4690-8578-d071e9e36377",
   "metadata": {},
   "source": [
    "- the chunk first preference based on sepeartor only\n",
    "\n",
    "- below examples for every new line we have more than 20 tokens \n",
    "\n",
    "- we provided chunk size of 20 tokens,eventhough becuase of seperater the chunk size is more than 20\n",
    "\n",
    "- this indicates this method will not take care of sentences and boundaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2ce6235-c01d-4d70-99c0-95b07a07e49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 188, which is longer than the specified 20\n",
      "Created a chunk of size 140, which is longer than the specified 20\n",
      "Created a chunk of size 166, which is longer than the specified 20\n",
      "Created a chunk of size 146, which is longer than the specified 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.',\n",
       " 'It provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation.',\n",
       " 'LangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps.',\n",
       " 'This chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Splitter=CharacterTextSplitter(separator='\\n',\n",
    "                              chunk_size=20, # least preference\n",
    "                              chunk_overlap=2)\n",
    "chunks=Splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba48d387-c093-4b41-80b9-71fae1f4b7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecc49cd2-1c91-4c06-a2e5-42f54269fc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how are you.Im good']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Splitter=CharacterTextSplitter(separator='\\n',\n",
    "                              chunk_size=5, # least preference\n",
    "                              chunk_overlap=2)\n",
    "chunks=Splitter.split_text('hello how are you.Im good')\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5380962a-7f7d-4248-b0e5-3b2cbbefb394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 188, which is longer than the specified 5\n",
      "Created a chunk of size 140, which is longer than the specified 5\n",
      "Created a chunk of size 166, which is longer than the specified 5\n",
      "Created a chunk of size 146, which is longer than the specified 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "20\n",
      "29\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "t1='LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.'\n",
    "from nltk.tokenize import word_tokenize\n",
    "for t1 in Splitter.split_text(text):\n",
    "    print(len(word_tokenize(t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363181b-50dc-4e58-927e-ad733f7b82e3",
   "metadata": {},
   "source": [
    "**chunk size means charcters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a6a6f8f-a485-4277-b80d-7c87b159818e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is an ope',\n",
       " 'pen-source framework',\n",
       " 'rk designed to facil',\n",
       " 'ilitate the developm',\n",
       " 'pment of application',\n",
       " 'ons that integrate l',\n",
       " 'large language mode',\n",
       " 'dels (LLMs) with ext',\n",
       " 'xternal data sources',\n",
       " 'es and computational',\n",
       " 'al logic.\\nIt provide',\n",
       " 'des tools, abstracti',\n",
       " 'tions, and component',\n",
       " 'nts to help develope',\n",
       " 'pers build language',\n",
       " 'e model-powered appl',\n",
       " 'plications beyond si',\n",
       " 'simple text generati',\n",
       " 'tion. \\nLangChain foc',\n",
       " 'ocuses on enabling c',\n",
       " 'chains of operation',\n",
       " 'ons, where outputs f',\n",
       " 'from an LLM can tri',\n",
       " 'rigger additional lo',\n",
       " 'logic, database quer',\n",
       " 'eries, API calls, or',\n",
       " 'or document retrieva',\n",
       " 'val steps. \\nThis cha',\n",
       " 'haining approach all',\n",
       " 'llows LLMs to perfor',\n",
       " 'orm multi-step reaso',\n",
       " 'soning, question-ans',\n",
       " 'nswering, summarizat',\n",
       " 'ation, and decision-',\n",
       " 'n-making over comple',\n",
       " 'lex workflows.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\n",
    "It provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \n",
    "LangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \n",
    "This chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.\n",
    " \"\"\"\n",
    "Splitter=CharacterTextSplitter(separator='',\n",
    "                              chunk_size=20,\n",
    "                              chunk_overlap=2)\n",
    "chunks=Splitter.split_text(text)\n",
    "chunks\n",
    "\n",
    "# 'LangChain is an ope'\n",
    "    # over lap is 2 charcters\n",
    "# 'pen-source framework'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "717d0cc3-5633-4218-8b4b-6f34b92c2feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('LangChain is an ope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c502ed2d-c5bf-4651-b0fe-ea160e7519de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('pen-source framework')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "338b91e3-6fb9-47ce-a5a8-e8fd7b6fb574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 188, which is longer than the specified 50\n",
      "Created a chunk of size 140, which is longer than the specified 50\n",
      "Created a chunk of size 166, which is longer than the specified 50\n",
      "Created a chunk of size 146, which is longer than the specified 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.',\n",
       " 'It provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation.',\n",
       " 'LangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps.',\n",
       " 'This chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"\n",
    "LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\n",
    "It provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \n",
    "LangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \n",
    "This chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.\n",
    " \"\"\"\n",
    "Splitter=CharacterTextSplitter(separator='\\n',\n",
    "                              chunk_size=50,\n",
    "                              chunk_overlap=2)\n",
    "chunks=Splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ea4f2-7aab-4553-a820-d3b38c5aa2fa",
   "metadata": {},
   "source": [
    "**why we need overlap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822bc4b-0f76-416e-b97d-c1d494c23d9a",
   "metadata": {},
   "source": [
    "- we seen it is splitting randomly based on seperator and based on chunk\n",
    "\n",
    "- the senetenece is breaking\n",
    "\n",
    "- model sees one chunk at a time and convert into numerical format\n",
    "\n",
    "- if we add overlap the end of the chunk is repated at the start of text\n",
    "\n",
    "- Context will be continue\n",
    "\n",
    "- Better retrieval process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b9c82-2825-47fe-bc0d-c49e85fd937a",
   "metadata": {},
   "source": [
    "**Recursive Text Splitter**\n",
    "\n",
    "- Recursive means hiercachy means step by step process\n",
    "\n",
    "- It first tries split by paragraphs(\\n\\n).\n",
    "\n",
    "- If the chunk is still too long it splits by sentence(.)\n",
    "\n",
    "- If still long then by space or charcter\n",
    "\n",
    "- Ensure chunks are within the given size but keeps context (using overlap)\n",
    "\n",
    "- Paragraph ====> Sentence ==== > words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a43d680d-e47e-44c0-b017-8ffc3e5bd69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is an open-source framework designed to',\n",
       " 'facilitate the development of applications that',\n",
       " 'integrate large language models (LLMs) with',\n",
       " 'external data sources and computational logic.',\n",
       " 'It provides tools, abstractions, and components',\n",
       " 'to help developers build language model-powered',\n",
       " 'applications beyond simple text generation.',\n",
       " 'LangChain focuses on enabling chains of',\n",
       " 'operations, where outputs from an LLM can trigger',\n",
       " 'additional logic, database queries, API calls, or',\n",
       " 'document retrieval steps.',\n",
       " 'This chaining approach allows LLMs to perform',\n",
       " 'multi-step reasoning, question-answering,',\n",
       " 'summarization, and decision-making over complex',\n",
       " 'workflows.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text=\"\"\"\n",
    "LangChain is an open-source framework designed to facilitate the development of applications that integrate large language models (LLMs) with external data sources and computational logic.\n",
    "It provides tools, abstractions, and components to help developers build language model-powered applications beyond simple text generation. \n",
    "LangChain focuses on enabling chains of operations, where outputs from an LLM can trigger additional logic, database queries, API calls, or document retrieval steps. \n",
    "This chaining approach allows LLMs to perform multi-step reasoning, question-answering, summarization, and decision-making over complex workflows.\n",
    " \"\"\"\n",
    "Splitter=RecursiveCharacterTextSplitter(separators='',\n",
    "                              chunk_size=50,\n",
    "                              chunk_overlap=2)\n",
    "chunks=Splitter.split_text(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a58d6c2b-47e7-4a17-bfc0-13932243a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db954135-fd41-44dc-a981-759cfc453efc",
   "metadata": {},
   "source": [
    "- paragraph\n",
    "\n",
    "- sentence\n",
    "\n",
    "- spaces\n",
    "\n",
    "- empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97825d3-45e3-4d63-8760-5316fd922702",
   "metadata": {},
   "outputs": [],
   "source": [
    "seperators=['\\n\\n','.',\" \",\"\"]\n",
    "text=\"\"\"Pragraph one: This is the first paragraph.It has two sentences.\n",
    "Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.It has several sentences to ensure splitting.Here another\n",
    "sentence to make it longer.\n",
    "Pragraph three: Short.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59e6eeb-f546-4056-a57c-467b8fe9857a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"\"\"Pragraph one: This is the first paragraph.It has two sentences.\n",
    "Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.It has several sentences to ensure splitting.Here another\n",
    "sentence to make it longer.\n",
    "Pragraph three: Short.\"\"\"\n",
    "\n",
    "# Paragraph level '/n/n'\n",
    "len(\"Pragraph one: This is the first paragraph.It has two sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c3bead-9a90-4396-936e-0b11fdde3da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.It has several sentences to ensure splitting.Here another\n",
    "sentence to make it longer.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aeb1823-dc4a-4f16-bed4-18a7186b5da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Pragraph three: Short.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325055a8-9ca5-4296-9337-02d17d18ae34",
   "metadata": {},
   "source": [
    "if will it split at paragraph level we have following chunk size\n",
    "\n",
    "- 63, 213 , 22\n",
    "\n",
    "- but if we provide chunk size has 50 only\n",
    "\n",
    "- then paragraph1 and paragph2 are large so furthure splitting will happend\n",
    "\n",
    "    - that furthure split based on sentences "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e0d6d-180e-4731-a7dc-640df168cbe2",
   "metadata": {},
   "source": [
    "**Sentence level slpit understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2bb0a62-95f0-459d-a776-8f9bd37788cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Pragraph one: This is the first paragraph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46df00ff-f960-458f-9a25-c61856fb713e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Pragraph one: This is the first paragraph.\"),len(\"It has two sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b957c966-0f41-407e-8097-3ee00b9fa501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 45, 40)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.\"\"\"),len(\"\"\"It has several sentences to ensure splitting.\"\"\"),len(\"\"\"Here another\n",
    "sentence to make it longer.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041283d-cd67-4c91-b5d4-243b4a6232af",
   "metadata": {},
   "source": [
    "- Now first paragraph does not have any problem , it will divide into two chunks\n",
    "\n",
    "    - 42,21 and both are less than 50\n",
    "\n",
    "- Second one splitted into 3 chunks\n",
    "\n",
    "    - 128, 45 and 40. but first chunk has 128 char it is more than 50\n",
    "\n",
    "    - So furthure split require\n",
    "\n",
    "    - furthure splitting means space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8c6fd1-6fee-4ca7-91fa-b4bbaf8880f2",
   "metadata": {},
   "source": [
    "**Space level split**\n",
    "\n",
    "- logically there are so many spaces will available\n",
    "\n",
    "- it will concetrate till max char to reach 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfd4af93-2070-4ef5-b69d-2fd68534063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"\"\"Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.\"\"\")\n",
    "\n",
    "len(\"Paragraph two: This is the second paragrpah which\") #49\n",
    "len(\"is quite long and will exceed our chunk size\")      #44\n",
    "len(\"limit for demenostration purpose.\") # 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d423200-62db-40a3-9c3c-cf7ca5c9bc66",
   "metadata": {},
   "source": [
    "**final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed06818-6fa7-4324-a09e-1bef8260be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"Pragraph one: This is the first paragraph.\") # 42\n",
    "len(\"It has two sentences.\") # 21\n",
    "len(\"Paragraph two: This is the second paragrpah which\") #49\n",
    "len(\"is quite long and will exceed our chunk size\")      #44\n",
    "len(\"limit for demenostration purpose.\") # 33\n",
    "len(\"\"\"It has several sentences to ensure splitting.\"\"\") # 45\n",
    "len(\"\"\"Here another sentence to make it longer.\"\"\") # 40\n",
    "len(\"Pragraph three: Short.\") # 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6269ba5-bd83-4509-9f34-571647d71fc1",
   "metadata": {},
   "source": [
    "**chunk size after splitting should be less than or equal to provided chunk size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cc881f4-998a-4358-a702-85c2c3ed087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "seperators=['\\n\\n','.',\" \",\"\"]\n",
    "text=\"\"\"Pragraph one: This is the first paragraph.It has two sentences.\n",
    "\n",
    "Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.It has several sentences to ensure splitting.Here another\n",
    "sentence to make it longer.\n",
    "\n",
    "Pragraph three: Short.\"\"\"\n",
    "\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=50,\n",
    "                              chunk_overlap=0,\n",
    "                              separators=seperators)\n",
    "\n",
    "chunks=splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "682a67a8-e2ba-4c8c-b182-ed3c066676db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pragraph one: This is the first paragraph',\n",
       " '.It has two sentences.',\n",
       " 'Paragraph two: This is the second paragrpah',\n",
       " 'which is quite long and will exceed our chunk',\n",
       " 'size limit\\nfor demenostration purpose',\n",
       " '.It has several sentences to ensure splitting',\n",
       " '.Here another\\nsentence to make it longer.',\n",
       " 'Pragraph three: Short.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10dbee5a-4b25-4e95-85a8-bf84702a74ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 has 41 chars\n",
      "Chunk 1 has 21 chars\n",
      "Chunk 2 has 45 chars\n",
      "Chunk 3 has 45 chars\n",
      "Chunk 4 has 37 chars\n",
      "Chunk 5 has 45 chars\n",
      "Chunk 6 has 40 chars\n",
      "Chunk 7 has 24 chars\n"
     ]
    }
   ],
   "source": [
    "for i,c in enumerate(chunks):\n",
    "    print(f\"Chunk {i} has {len(c)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "176721bd-5d5d-46d1-9c7e-c6792af077fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "seperators=['\\n\\n']\n",
    "text=\"\"\"Pragraph one: This is the first paragraph.It has two sentences.\n",
    "Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.It has several sentences to ensure splitting.Here another\n",
    "sentence to make it longer.\n",
    "Pragraph three: Short.\"\"\"\n",
    "\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=50,\n",
    "                              chunk_overlap=0,\n",
    "                              separators=seperators)\n",
    "\n",
    "chunks=splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2655770f-006d-47dd-8b21-be05a738e568",
   "metadata": {},
   "source": [
    "## TokenTextSplitter\n",
    "\n",
    "- If we want to chunks based on **tokens** then we use **TokenTextSplitter**\n",
    "\n",
    "- 'Hello' is one token, but 'Hello' has 5 charcters\n",
    "\n",
    "- CharcterTextSplitter and RecursiveCharcterText splitter based on charcters only\n",
    "\n",
    "- But TokenTextSplitter completely based on token only\n",
    "\n",
    "    - tokens can be words,spl char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dd53c-3374-43dc-bb84-ad611b2931f4",
   "metadata": {},
   "source": [
    "**tiktoken**\n",
    "\n",
    "- Official OpenAI tokenizer\n",
    "\n",
    "- it converts text to tokens and tokens to text\n",
    "\n",
    "- Count tokens in text\n",
    "\n",
    "- Split texts so that it fits within token limits\n",
    "\n",
    "- Estimate cost\n",
    "\n",
    "- pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c270351-80ce-46c1-a4e4-eaeadc1179f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding=tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8947c903-1a37-4516-9b5f-66873f93a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 3021, 6975, 23272, 8995, 323, 15592, 0]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"I love learning Langchain and AI!\"\n",
    "tokens=encoding.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bae561af-6254-4f88-86b0-218a822a9b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "279e95d4-817d-4177-81f7-681f668eebe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beeb6cfd-ffd0-494b-9af8-93a65171d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5317, 8995], 'lang')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode('langchain'), encoding.decode([5317])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e2e76a-9d9f-4875-b978-c0d21ae1c178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love learning Langchain and AI!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44480cf-34d5-43e5-9ba2-56d11b0747dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I love learning Lnagchain and AI!\"=['I','love','learning','langchain','and','AI','!']  # 7\n",
    "[40, 3021, 6975, 98919, 351, 8995, 323, 15592, 0] # 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6226551-5cc2-4f1b-8d50-10f142d6e152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c6f9609-4316-49c8-bd03-c20051ffba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 I\n",
      "3021  love\n",
      "6975  learning\n",
      "23272  Lang\n",
      "8995 chain\n",
      "323  and\n",
      "15592  AI\n",
      "0 !\n"
     ]
    }
   ],
   "source": [
    "word=\"I love learning Langchain and AI!\"\n",
    "t=encoding.encode(word)\n",
    "for i in t:\n",
    "    print(i,encoding.decode([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d36f892e-62da-4cad-a3e6-523394014d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21656]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ad9a9b1-cecf-4675-9734-5d4b955ef5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "text=\"\"\"Pragraph one: This is the first paragraph.It has two sentences.\n",
    "Paragraph two: This is the second paragrpah which is quite long and will exceed our chunk size limit\n",
    "for demenostration purpose.It has several sentences to ensure splitting.Here another\n",
    "sentence to make it longer.\n",
    "Pragraph three: Short.\"\"\"\n",
    "\n",
    "\n",
    "splitter=TokenTextSplitter(chunk_size=15, \n",
    "                              chunk_overlap=3)\n",
    "# each chunk will have 15 tokens \n",
    "# each token overlap of 3 tokens\n",
    "\n",
    "chunks=splitter.split_text(text)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f9656f4-1463-466e-b0ef-23dfbaefb940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pragraph one: This is the first paragraph.It has two sentences.',\n",
       " ' two sentences.\\nParagraph two: This is the second paragr',\n",
       " ' paragrpah which is quite long and will exceed our chunk size',\n",
       " ' our chunk size limit\\nfor demenostration purpose.It has several',\n",
       " 'It has several sentences to ensure splitting.Here another\\nsentence to make',\n",
       " 'ence to make it longer.\\nPragraph three: Short.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "816857f2-2bcb-4e94-ba98-7c112bbd8ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a025012-88ba-4be7-aeac-6d3f16f7f0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' paragrpah which is quite long and will exceed our chunk size'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbdac62c-61aa-4320-a6e9-590c0062a2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "28531 dem\n",
      "12052 eno\n",
      "55681 stration\n"
     ]
    }
   ],
   "source": [
    "word=\"demenostration\"\n",
    "t=encoding.encode(word)\n",
    "print(len(t))\n",
    "for i in t:\n",
    "    print(i,encoding.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59e830-1f97-4a74-9fdc-be7f8337c2a4",
   "metadata": {},
   "source": [
    "- tokenTextsplitter Boundaries based on token only\n",
    "\n",
    "- demenostration divides into 3 tokens so there is a chance the chunk can break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f5f74cb-065d-4cb6-91e7-62bc8630179a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 : ChatGPT helps developers build powerful AI has ===> 42 chars\n",
      "Chunk 1 : applications efficiently has ===> 24 chars\n",
      "Chunk 2 : .\n",
      "      It provideds tools and APIs that make has ===> 45 chars\n",
      "Chunk 3 : integraton easy and scalable has ===> 28 chars\n",
      "============Tik token =========================\n",
      "Chunk 0 : ChatGPT helps developers build powerful AI applications efficiently has ===> 67 chars\n",
      "Chunk 1 :  applications efficiently.\n",
      "      It has ===> 35 chars\n",
      "Chunk 2 :   It provideds tools and APIs that make integ has ===> 45 chars\n",
      "Chunk 3 :  make integraton easy and scalable has ===> 34 chars\n",
      "Chunk 0 has 10 tokens : [16047, 38, 2898, 8779, 13707, 1977, 8147, 15592, 8522, 30820]\n",
      "Chunk 1 has 5 tokens : [8522, 30820, 627, 415, 1102]\n",
      "Chunk 2 has 10 tokens : [220, 1102, 3984, 82, 7526, 323, 34456, 430, 1304, 5503]\n",
      "Chunk 3 has 6 tokens : [1304, 8936, 24444, 4228, 323, 69311]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter,RecursiveCharacterTextSplitter\n",
    "text=\"\"\"ChatGPT helps developers build powerful AI applications efficiently.\n",
    "      It provideds tools and APIs that make integraton easy and scalable\"\"\"\n",
    "\n",
    "seperators=['\\n\\n','.',\" \",\"\"]\n",
    "recursive_splitter=RecursiveCharacterTextSplitter(chunk_size=50,\n",
    "                              chunk_overlap=0,\n",
    "                              separators=seperators)\n",
    "\n",
    "recursive_chunks=recursive_splitter.split_text(text)\n",
    "len(recursive_chunks)\n",
    "\n",
    "for i,c in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i} : {c} has ===> {len(c)} chars\")\n",
    "######################################################################\n",
    "print('============Tik token =========================')\n",
    "token_splitter=TokenTextSplitter(chunk_size=10, \n",
    "                              chunk_overlap=2)\n",
    "# each chunk will have 15 tokens \n",
    "# each token overlap of 3 tokens\n",
    "\n",
    "token_chunks=token_splitter.split_text(text)\n",
    "len(token_chunks)\n",
    "\n",
    "for i,c in enumerate(token_chunks):\n",
    "    print(f\"Chunk {i} : {c} has ===> {len(c)} chars\")\n",
    "\n",
    "##########################################################################\n",
    "enc=tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "#encoding=tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "for i,c in enumerate(token_chunks):\n",
    "    tokens=enc.encode(c)\n",
    "    print(f\"Chunk {i} has {len(tokens)} tokens : {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b50807-0125-4900-96bc-27c48e9a9f2a",
   "metadata": {},
   "source": [
    "- RecursiveTextsplitter for normal language documents we can use\n",
    "\n",
    "    - Natural readabiity\n",
    "\n",
    "    - chunking boundaries based on senetence or paragraph\n",
    "\n",
    "    - here we are not worrying token limit\n",
    "\n",
    "- if we feel the token limits are more important becuase this directly impact to cost\n",
    "\n",
    "    - so at that time go with token text splitter\n",
    "\n",
    "    - Chunk boundary is based on tokens then token text splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573eb8d-af1c-4fd5-833d-282353437f50",
   "metadata": {},
   "source": [
    "## Semantic Embedding Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194ac6d-43f1-424d-b846-8d5e3238ece8",
   "metadata": {},
   "source": [
    "- **Semantic Embedding chunking** is a strategy used when preparing text data for Embedding models\n",
    "\n",
    "- So that long documents can be effectively represented as smaller, meaningful chunks \n",
    "\n",
    "- It preserves **Semantic Coherence**\n",
    "\n",
    "    - Coherence means a logical and consistent connection where all parts of a whole fit together well\n",
    "\n",
    "- Each chunk remains self contained and meaningful\n",
    "\n",
    "- It involves 3 main steps\n",
    "\n",
    "- 1. **Structure-aware Segmentation**\n",
    "\n",
    "    - Use documents structure (headings, paragraphs , bullet points) as initial boundaries\n",
    "\n",
    "    - Example: Split by Markdown boundaries\n",
    "\n",
    "    - Split by Paragraphs or Split by HTML tags etc\n",
    "\n",
    "- 2. **Semantic similarity-based merging**\n",
    "\n",
    "    - Embed the sentences or paragraphs first\n",
    "\n",
    "    - Embed means a vector representation of numbers\n",
    "\n",
    "    - it will compute similarity metric (e.g. cosine similarity) betwen sentences or paragraphs\n",
    "\n",
    "    - Ex1: we have senetnce1 and Sentence2 divided\n",
    "\n",
    "    - then start compute the similarity metric between two senetences\n",
    "\n",
    "    - Merge consecutive sentences or paragraphs that are semantically close each until the tokne limit will reach\n",
    "\n",
    "- 3. **Overlap and Context preservation**\n",
    "\n",
    "    - Use small overlaps betwen adjacent chunks to prserve context so that It will help in rerieval in RAG applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8a94d0-d96d-4e13-ad5c-2775abc04eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are so many embedding models are available \n",
    "# Embedding model: SentenceTransformer \n",
    "\n",
    "# pip install sentence-transformers\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d312a00-3ebb-4dd5-a93b-eeeecbe5aa77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model  # 384 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59482f21-40bc-4186-bf8d-40347b67f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "Coherence means a logical and consistent connection where all parts of a whole fit together well. \n",
    "In a general sense, it refers to things like an argument, story, or plan being easy to understand because \n",
    "it is logically integrated and consistent. \n",
    "In physics, coherence describes waves that have a stable, fixed relationship in their phase and frequency, \n",
    "which is essential for creating stable interference patterns. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e89466-eda3-441a-9a4f-ba53ea8f6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f50e9cd8-297a-44c1-a2af-b4b906a6436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "embeddings=model.encode(sent,convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e264e52a-d588-46aa-8d92-b0992d1e27fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1b89ad-ba77-4d1b-9d1b-343b18573060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd35711-d781-4d9a-aa9d-4ac666e2bce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44335645]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relation between embeddings[0] ===> sen1  and embeddings[1] ===> sen2\n",
    "cosine_similarity([embeddings[0]],[embeddings[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c3804c-844f-44be-9a3f-82afc01bcb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7643918]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([embeddings[0]],[embeddings[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5798ed-d07b-477f-a2da-b5d1c8d55aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So Sente1 and sent3 should be merge \n",
    "# Semantic chunker method from langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c8b71-181d-42d6-ba62-a1be35e1ad55",
   "metadata": {},
   "source": [
    "**Semantic Chunk using langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d92cee-b75e-4675-a1a3-61d13b2abc19",
   "metadata": {},
   "source": [
    "- step-1: we will read the embedd model\n",
    "\n",
    "    - Every LLM has related embedd model availabe\n",
    "\n",
    "    - OpenAI has its own embedd model (With API key)\n",
    "\n",
    "    - GeminiAI has its own embedd model (with API key)\n",
    "\n",
    "    - hugging face transformers embedd model its works offline (with out API key)\n",
    "\n",
    "- Step-2: Pass that embedd model the splitter i.e. semantic chunk that we will take by langchain\n",
    "\n",
    "- Step-3: Now pass the text we will get chunks based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb72185-6266-4e0e-ab13-39fe0d8e4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-experimental\n",
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "973f6a90-0a17-48d4-b776-0d680b4cbadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# You will need to install a library for embeddings, such as sentence-transformers\n",
    "# pip install sentence-transformers\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# Create an instance of the embedding model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22fc0a7b-9d1f-4e34-b577-62418b8fa220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_experimental.text_splitter.SemanticChunker at 0x20e1d1a68a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the SemanticChunker\n",
    "# pip install langchain-experimental\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "semantic_chunker = SemanticChunker(embeddings)\n",
    "semantic_chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b596c-3e54-44f7-b731-73fcb06120c2",
   "metadata": {},
   "source": [
    "- CharcterTextSplitter does not required any model\n",
    "\n",
    "- RecursiveTextSplotter does not required any model\n",
    "\n",
    "- TikTokenTextSplitter  internally use GPT model\n",
    "\n",
    "- SemanticChunker need any embedd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25b48694-c832-459b-9d4f-b72c098f3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "\n",
      "AI has come a long way since 1952, when the first documented success of an AI computer program was written by Christopher Strachey, whose checkers program completed a whole game on the Ferranti Mark I computer at the University of Manchester. Thanks to developments in machine learning and deep learning, IBMs Deep Blue defeated chess grandmaster Garry Kasparov in 1997, and the companys IBM Watson won Jeopardy! in 2011.\n",
      "---\n",
      "Chunk 2:\n",
      "Since then, generative AI has spearheaded the latest chapter in AIs evolution, with OpenAI releasing its first GPT model in 2018. This has culminated in OpenAI developing ChatGPT, leading to a proliferation of tools that can process queries to produce relevant text, audio, images and other types of content. Other companies have followed suit with competing products of their own, including Googles Gemini, Anthropics Claude and DeepSeeks R1 and V3 models, which made headlines in early 2025 for approaching parity with competing models at a fraction of the operational cost. AI has also been used to help sequence RNA for vaccines and model human speech, technologies that rely on model- and algorithm-based machine learning and increasingly focus on perception, reasoning and generalization. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Example text to split\n",
    "text = \"\"\"\n",
    "AI has come a long way since 1952, when the first documented success of an AI computer program was written by Christopher Strachey, whose checkers program completed a whole game on the Ferranti Mark I computer at the University of Manchester. Thanks to developments in machine learning and deep learning, IBMs Deep Blue defeated chess grandmaster Garry Kasparov in 1997, and the companys IBM Watson won Jeopardy! in 2011.  \n",
    "\n",
    "Since then, generative AI has spearheaded the latest chapter in AIs evolution, with OpenAI releasing its first GPT model in 2018. This has culminated in OpenAI developing ChatGPT, leading to a proliferation of tools that can process queries to produce relevant text, audio, images and other types of content.\n",
    "\n",
    "Other companies have followed suit with competing products of their own, including Googles Gemini, Anthropics Claude and DeepSeeks R1 and V3 models, which made headlines in early 2025 for approaching parity with competing models at a fraction of the operational cost.\n",
    "\n",
    "AI has also been used to help sequence RNA for vaccines and model human speech, technologies that rely on model- and algorithm-based machine learning and increasingly focus on perception, reasoning and generalization.\n",
    "\"\"\"\n",
    "\n",
    "# Split the text\n",
    "chunks = semantic_chunker.split_text(text)\n",
    "#print(chunks)\n",
    "# Print the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1a3b2-e5a0-40aa-83ab-554a3e86c5b6",
   "metadata": {},
   "source": [
    "- SemanticChunker works a bit differently than other text splitters you might be familiar with, like RecursiveCharacterTextSplitter. \n",
    "\n",
    "- Instead of splitting based on a fixed token limit or character count, SemanticChunker splits text based on the semantic similarity of the sentences.\n",
    "\n",
    "- There isn't a direct chunk_size or chunk_overlap parameter like in other text splitters. \n",
    "\n",
    "- The splitting behavior is primarily controlled by the breakpoint_threshold_type and breakpoint_threshold parameters during initialization. \n",
    "\n",
    "- These parameters determine how dissimilar the semantic embeddings of sentences need to be to create a split.\n",
    "\n",
    "- A lower breakpoint_threshold will result in more frequent splits and therefore smaller chunks, while a higher threshold will result in fewer splits and larger chunks.\n",
    "\n",
    "- So, while you don't control the token limit directly, you can indirectly influence the size of the chunks by adjusting the breakpoint_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da702fab-ee0d-49b2-985f-ed36d4670e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "\n",
      "AI has come a long way since 1952, when the first documented success of an AI computer program was written by Christopher Strachey, whose checkers program completed a whole game on the Ferranti Mark I computer at the University of Manchester. Thanks to developments in machine learning and deep learning, IBMs Deep Blue defeated chess grandmaster Garry Kasparov in 1997, and the companys IBM Watson won Jeopardy! in 2011.\n",
      "---\n",
      "Chunk 2:\n",
      "Since then, generative AI has spearheaded the latest chapter in AIs evolution, with OpenAI releasing its first GPT model in 2018. This has culminated in OpenAI developing ChatGPT, leading to a proliferation of tools that can process queries to produce relevant text, audio, images and other types of content. Other companies have followed suit with competing products of their own, including Googles Gemini, Anthropics Claude and DeepSeeks R1 and V3 models, which made headlines in early 2025 for approaching parity with competing models at a fraction of the operational cost. AI has also been used to help sequence RNA for vaccines and model human speech, technologies that rely on model- and algorithm-based machine learning and increasingly focus on perception, reasoning and generalization. \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Split the text\n",
    "chunks = semantic_chunker.split_text(text)\n",
    "\n",
    "# Print the chunks\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c5225-81c8-491a-bda7-83d5e04519ac",
   "metadata": {},
   "source": [
    "**Problem**\n",
    "\n",
    "- This chunks divided based on semantic meaning of sentences \n",
    "\n",
    "- One chunk means this all the data has a meaning \n",
    "\n",
    "- But different chunks has different sizes\n",
    "\n",
    "- We are not able to control the tokens in each chunk\n",
    "\n",
    "- Some times we might face the issue with our context window or model max token length might be reached\n",
    "\n",
    "- then you need to take these chunks and apply **Recursive Text Splitter** \n",
    "\n",
    "- we can control the token limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601e858d-89ae-41d8-bafe-2f81c59cdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5dda5d5-485b-4ad4-bec8-ed81753e50f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CharacterTextSplitter',\n",
       " 'ElementType',\n",
       " 'HTMLHeaderTextSplitter',\n",
       " 'HeaderType',\n",
       " 'KonlpyTextSplitter',\n",
       " 'Language',\n",
       " 'LatexTextSplitter',\n",
       " 'LineType',\n",
       " 'MarkdownHeaderTextSplitter',\n",
       " 'MarkdownTextSplitter',\n",
       " 'NLTKTextSplitter',\n",
       " 'PythonCodeTextSplitter',\n",
       " 'RecursiveCharacterTextSplitter',\n",
       " 'RecursiveJsonSplitter',\n",
       " 'SentenceTransformersTokenTextSplitter',\n",
       " 'SpacyTextSplitter',\n",
       " 'TextSplitter',\n",
       " 'TokenTextSplitter',\n",
       " 'Tokenizer',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'split_text_on_tokens']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d031ef7-f469-4d0a-809c-24149f56dda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
